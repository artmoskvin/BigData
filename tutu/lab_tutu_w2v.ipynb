{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('penalties_train.json', 'r') as f:\n",
    "    raw_data = f.readlines()\n",
    "    \n",
    "raw_data = map(lambda x: x.rstrip(), raw_data)\n",
    "raw_data_str = '[' + ','.join(raw_data) + ']'\n",
    "labeled_train_data = pd.read_json(raw_data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('unmarked.json', 'r') as f:\n",
    "    raw_data = f.readlines()\n",
    "    \n",
    "raw_data = map(lambda x: x.rstrip(), raw_data)\n",
    "raw_data_str = '[' + ','.join(raw_data) + ']'\n",
    "unlabeled_train_data = pd.read_json(raw_data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('penalties_test.json', 'r') as f:\n",
    "    raw_data = f.readlines()\n",
    "    \n",
    "raw_data = map(lambda x: x.rstrip(), raw_data)\n",
    "raw_data_str = '[' + ','.join(raw_data) + ']'\n",
    "test_data = pd.read_json(raw_data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11753\n",
      "10014\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "print len(unlabeled_train_data)\n",
    "print len(labeled_train_data)\n",
    "print len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sample</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 14</td>\n",
       "      <td> BETWEEN RUSSIA AND EUROPE\\n  CHANGES\\n    CHAN...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 17</td>\n",
       "      <td> BETWEEN RUSSIA AND AREA 3\\n  CHANGES\\n    CHAN...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 21</td>\n",
       "      <td> FOR NEX  FARES\\n  CANCELLATIONS\\n    BEFORE DE...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 33</td>\n",
       "      <td> BETWEEN MSQ AND MOW FOR ROUND TRIP FARES\\n  CA...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 35</td>\n",
       "      <td> UNLESS OTHERWISE SPECIFIED\\n  FOR TRAVEL ON/BE...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             sample  token\n",
       "0  14  BETWEEN RUSSIA AND EUROPE\\n  CHANGES\\n    CHAN...    NaN\n",
       "1  17  BETWEEN RUSSIA AND AREA 3\\n  CHANGES\\n    CHAN...    NaN\n",
       "2  21  FOR NEX  FARES\\n  CANCELLATIONS\\n    BEFORE DE...    NaN\n",
       "3  33  BETWEEN MSQ AND MOW FOR ROUND TRIP FARES\\n  CA...    NaN\n",
       "4  35  UNLESS OTHERWISE SPECIFIED\\n  FOR TRAVEL ON/BE...    NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unlabeled_sample = unlabeled_train_data['sample'].tolist()\n",
    "labeled_sample = labeled_train_data['sample'].tolist()\n",
    "test_sample = test_data['sample'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Токенизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "r = RegexpTokenizer('\\w+')\n",
    "stop_words_ = stopwords.words('english') + stopwords.words('russian')\n",
    "\n",
    "def processing_text(text):\n",
    "    raw_sentences = sent_tokenize(text.strip())\n",
    "    sentences = []\n",
    "    for raw in raw_sentences:\n",
    "        if len(raw) >0:\n",
    "            no_stop = [i for i in r.tokenize(raw.lower()) if i not in stop_words_] #uncommented after Word2Vec model been taught\n",
    "            sentences += no_stop #editted from .append to += after Word2Vec model done\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ok', 'ok', 'man']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_text('I was, ok but: this is! Not ok, man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = u'Привет, как дела?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442, \\u043a\\u0430\\u043a \\u0434\\u0435\\u043b\\u0430?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ok']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = 'I was, ok but: this not!'\n",
    "[i for i in r.tokenize(raw.lower()) if i not in stop_words_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Токенизатор Kaggle (проблемы с русским)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    return(words)\n",
    "\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences( review, tokenizer ):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( review_to_wordlist( raw_sentence ))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'was', 'ok', 'but', 'this', 'not'], ['get', 'off', 'man']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_sentences('I was, ok but: this not! Get off, man', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review = u'Привет, как дела?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\\u041f\\u0440\\u0438\\u0432\\u0435\\u0442, \\u043a\\u0430\\u043a \\u0434\\u0435\\u043b\\u0430?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(review.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = u'Привет, как дела?'\n",
    "re.sub(\"[^a-zA-Zа-яА-ЯёЁ]\",\" \", review).lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent = []\n",
    "for i in unlabeled_sample:\n",
    "    sent += processing_text(i)\n",
    "    \n",
    "for i in labeled_sample:\n",
    "    sent += processing_text(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "909923"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 13s, sys: 35.7 s, total: 6min 48s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(sent, workers=num_workers, size=num_features, \\\n",
    "                 min_count=min_word_count, \\\n",
    "                 window = context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'man'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot select a word from an empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-45a81fb10163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoesnt_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paris berlin london austria\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mdoesnt_match\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using words %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot select a word from an empty list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot select a word from an empty list"
     ]
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'itineraries', 0.593355655670166),\n",
       " (u'good', 0.4699913263320923),\n",
       " (u'ii', 0.4585806727409363),\n",
       " (u'website', 0.454235702753067),\n",
       " (u'counters', 0.4495977759361267),\n",
       " (u'rebooked', 0.4460126757621765),\n",
       " (u'commences', 0.444488525390625),\n",
       " (u'differ', 0.4353044927120209),\n",
       " (u'canada', 0.42385154962539673),\n",
       " (u'wholy', 0.41377708315849304)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"usa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3761, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.95222981e-02,  -9.25823525e-02,  -1.17884250e-02,\n",
       "        -2.70562228e-02,   1.09305941e-02,   8.45116153e-02,\n",
       "        -5.53298555e-02,  -6.25760257e-02,  -4.11186880e-03,\n",
       "        -3.68745178e-02,   7.81741273e-03,   1.50685473e-05,\n",
       "        -5.29269911e-02,   3.82249393e-02,  -5.52816354e-02,\n",
       "        -2.57438459e-02,   6.45049801e-03,  -3.97516042e-02,\n",
       "        -1.04024753e-01,   4.53763753e-02,   1.90500785e-02,\n",
       "         1.53786251e-02,   2.12360336e-03,   3.10129859e-03,\n",
       "        -9.06417668e-02,   3.66495214e-02,  -1.52401635e-02,\n",
       "         3.59585788e-03,   4.97317612e-02,  -4.94841039e-02,\n",
       "         1.43225752e-02,   2.04714239e-02,   1.39417220e-02,\n",
       "         1.54720172e-02,  -1.06012728e-02,   7.34348670e-02,\n",
       "         2.59272195e-02,   4.93043326e-02,  -6.37894645e-02,\n",
       "        -5.52719608e-02,   4.31292728e-02,   1.36197321e-02,\n",
       "        -5.89284021e-03,  -5.67552149e-02,   1.22092783e-01,\n",
       "        -1.07678629e-01,   4.22914587e-02,   2.59863175e-02,\n",
       "        -3.17546986e-02,   5.07033169e-02,  -8.74007680e-03,\n",
       "         1.16979152e-01,  -5.71275968e-03,   5.42159490e-02,\n",
       "         4.09802534e-02,  -3.86853702e-02,  -1.64839234e-02,\n",
       "        -1.10360654e-02,  -4.40284349e-02,   1.75192188e-02,\n",
       "        -1.99840646e-02,   1.30792812e-01,  -2.29903348e-02,\n",
       "         2.20956951e-02,  -1.82725973e-02,   2.93880468e-04,\n",
       "         3.56323533e-02,   1.13768108e-01,  -3.23353559e-02,\n",
       "        -1.57359540e-02,   7.54027590e-02,   2.80171707e-02,\n",
       "         6.62587816e-03,  -1.20773531e-01,  -1.84514701e-01,\n",
       "        -4.20059562e-02,   2.49980018e-02,   3.43903489e-02,\n",
       "         3.99024375e-02,   6.29379600e-03,   8.91135931e-02,\n",
       "         2.71264091e-02,   1.35383038e-02,  -5.91395386e-02,\n",
       "        -1.04297616e-03,   8.01522136e-02,   1.29011691e-01,\n",
       "         2.38012299e-02,   7.05846921e-02,   9.39802304e-02,\n",
       "        -1.14899248e-01,   3.54488604e-02,  -2.80215964e-02,\n",
       "        -4.28869948e-02,  -9.09049064e-02,  -1.14278402e-02,\n",
       "         1.05849974e-01,   8.16719308e-02,   5.30724004e-02,\n",
       "         9.84685123e-02,  -8.68767723e-02,   3.43313180e-02,\n",
       "         7.34823197e-02,  -4.91075218e-02,  -2.94726365e-03,\n",
       "         4.73592579e-02,  -3.13869230e-02,   1.01485047e-02,\n",
       "        -7.01157302e-02,  -2.85343193e-02,  -2.27474533e-02,\n",
       "        -1.05223492e-01,   3.75148878e-02,   1.53187830e-02,\n",
       "         8.27072337e-02,  -3.34286019e-02,  -2.11834703e-02,\n",
       "         2.70100944e-02,  -3.11005041e-02,   8.13378394e-02,\n",
       "        -5.16791716e-02,   1.15596272e-01,  -7.96528980e-02,\n",
       "        -1.47282369e-02,   4.41021286e-02,  -1.04009517e-01,\n",
       "         6.82594329e-02,   9.85315442e-03,  -5.58731779e-02,\n",
       "        -8.75571370e-02,  -1.09241232e-01,  -3.10761426e-02,\n",
       "        -2.11729556e-02,   2.37679947e-02,   1.26157925e-01,\n",
       "        -5.18724769e-02,   6.20198548e-02,  -4.79641603e-03,\n",
       "         3.11387423e-03,   1.06412679e-01,   2.07369812e-02,\n",
       "         2.85410024e-02,   1.06147029e-01,   6.67621866e-02,\n",
       "         4.10604849e-02,   5.88752050e-03,   3.79618295e-02,\n",
       "         2.57433038e-02,  -1.00905485e-01,  -3.20588574e-02,\n",
       "         8.00429881e-02,   6.83151186e-02,   6.59447312e-02,\n",
       "         2.08344907e-02,  -2.86203064e-02,   2.65887156e-02,\n",
       "        -8.85449946e-02,   1.13781923e-02,   6.20979490e-03,\n",
       "         5.22937439e-02,  -1.90465245e-04,   7.74973407e-02,\n",
       "        -9.25239176e-02,  -5.40985540e-03,  -3.74057665e-02,\n",
       "         1.05908485e-02,  -6.79737777e-02,  -5.06508071e-03,\n",
       "        -4.33859192e-02,  -4.49259803e-02,   5.98725267e-02,\n",
       "         8.20611268e-02,   1.88385770e-02,  -1.18033998e-01,\n",
       "         9.69837159e-02,   1.75224338e-02,  -3.96459289e-02,\n",
       "         3.65464725e-02,   5.04310764e-02,  -1.41297998e-02,\n",
       "         5.07203005e-02,  -6.66491978e-04,  -1.08460877e-02,\n",
       "         9.55865905e-03,   4.90700416e-02,   3.20028625e-02,\n",
       "        -7.25088362e-03,  -4.91971802e-03,   3.25581944e-03,\n",
       "        -9.07488167e-02,   4.74986099e-02,  -1.99153479e-02,\n",
       "         8.50896984e-02,   5.40105291e-02,   2.50951033e-02,\n",
       "         5.25046177e-02,  -1.74206421e-02,   4.12512384e-03,\n",
       "        -2.13128471e-04,   4.87209521e-02,  -3.10641564e-02,\n",
       "        -6.26561046e-02,  -8.38683173e-02,   1.37019921e-02,\n",
       "        -4.19643745e-02,   3.38725746e-02,  -8.53486732e-02,\n",
       "        -6.07745200e-02,   1.77589431e-02,  -6.60573393e-02,\n",
       "         6.22149603e-03,  -7.59418607e-02,  -2.42932048e-02,\n",
       "         5.82471602e-02,   3.49461590e-03,  -7.90260434e-02,\n",
       "        -1.08720899e-01,  -4.59296815e-02,  -5.60475104e-02,\n",
       "         3.81669495e-03,   1.78347182e-04,  -2.62958109e-02,\n",
       "        -9.51877888e-03,  -1.56898778e-02,   3.57258990e-02,\n",
       "        -8.48691016e-02,  -7.74414390e-02,   5.89536205e-02,\n",
       "         2.74222065e-03,  -1.81725845e-02,  -7.54664615e-02,\n",
       "         3.51615064e-02,   6.94278302e-03,  -3.94641981e-02,\n",
       "        -2.98062023e-02,   1.01163328e-01,   8.39665830e-02,\n",
       "        -3.85360681e-02,  -4.46072221e-02,  -1.58869755e-02,\n",
       "         6.59980252e-02,   4.73936088e-02,  -7.79512897e-02,\n",
       "         2.21964605e-02,  -3.13311480e-02,   6.78483769e-02,\n",
       "         4.54633087e-02,  -2.75172573e-02,  -4.75591198e-02,\n",
       "        -2.12194305e-03,   2.65921392e-02,  -1.19178956e-02,\n",
       "        -1.14931827e-02,   1.49900885e-02,  -1.25220850e-01,\n",
       "         2.53783260e-02,  -2.25139745e-02,   6.69310801e-04,\n",
       "         1.55607443e-02,   2.00899802e-02,   2.29178444e-02,\n",
       "        -1.06827222e-01,  -9.25331488e-02,  -8.21079463e-02,\n",
       "        -4.79586571e-02,  -1.59219303e-03,   1.81042273e-02,\n",
       "        -1.66189857e-02,   1.94350667e-02,  -2.93577965e-02,\n",
       "        -4.04473022e-02,   3.75901498e-02,  -2.51418613e-02,\n",
       "         5.05597591e-02,  -3.42294089e-02,   6.02998026e-02,\n",
       "        -5.19397520e-02,   1.64594743e-02,   2.38112500e-03,\n",
       "        -1.17717996e-01,  -4.61951084e-02,   9.56277847e-02,\n",
       "        -3.87497358e-02,  -1.19820796e-01,  -5.32706156e-02,\n",
       "         1.41047845e-02,   3.83162275e-02,  -5.59602343e-02,\n",
       "        -5.41490056e-02,   6.53032884e-02,   1.82199955e-01,\n",
       "        -8.34219381e-02,  -8.07499047e-03,   5.40990755e-02,\n",
       "        -2.99900137e-02,   6.68486506e-02,   4.09280956e-02,\n",
       "         1.68283626e-01,  -2.64425706e-02,  -1.01028338e-01], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Средние векторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, n_features):\n",
    "    f_v = np.zeros((n_features,), dtype=\"float32\")\n",
    "    n_w = 0.\n",
    "    index2word_set = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_w += 1\n",
    "            f_v = np.add(f_v, model[word])\n",
    "    f_v = np.divide(f_v, n_w)\n",
    "    return f_v\n",
    "\n",
    "def text_to_vec(texts, model, n_features):\n",
    "    i = 0.\n",
    "    text_v = np.zeros((len(texts), n_features), dtype=\"float32\")\n",
    "    for text in texts:\n",
    "        if i % 1000. == 0.:\n",
    "            print(i)\n",
    "        text_v[i] = makeFeatureVec(text, model, n_features)\n",
    "        i += 1\n",
    "    return text_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train_sample = []\n",
    "for sample in labeled_sample:\n",
    "    clean_train_sample.append(processing_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10014"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Функции Kaggle (то же самое)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0.\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "       if counter%1000. == 0.:\n",
    "           print \"Review %d of %d\" % (counter, len(reviews))\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10014\n",
      "Review 1000 of 10014\n",
      "Review 2000 of 10014\n",
      "Review 3000 of 10014\n",
      "Review 4000 of 10014\n",
      "Review 5000 of 10014\n",
      "Review 6000 of 10014\n",
      "Review 7000 of 10014\n",
      "Review 8000 of 10014\n",
      "Review 9000 of 10014\n",
      "Review 10000 of 10014\n"
     ]
    }
   ],
   "source": [
    "trainDataVecs = getAvgFeatureVecs( clean_train_sample, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_test_sample = []\n",
    "for sample in test_sample:\n",
    "    clean_test_sample.append(processing_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 2400\n",
      "Review 1000 of 2400\n",
      "Review 2000 of 2400\n"
     ]
    }
   ],
   "source": [
    "testDataVecs = getAvgFeatureVecs( clean_test_sample, model, num_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit(trainDataVecs, labeled_train_data['token'] )\n",
    "\n",
    "result_rf = forest.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "test_data['token'] = result_rf\n",
    "for i in range(len(test_data)):\n",
    "    result = {}\n",
    "    result['id'] = str(test_data['id'][i])\n",
    "    result['sample'] = test_data['sample'][i]\n",
    "    result['token'] = test_data['token'][i]\n",
    "    if i < 2399:\n",
    "        with open('w2v_avg_rf.json', 'a') as f:\n",
    "            json.dump(result, f)\n",
    "            f.write('\\n')\n",
    "    else:\n",
    "        with open('w2v_avg_rf.json', 'a') as f:\n",
    "            json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.6125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "lr = lr.fit(trainDataVecs, labeled_train_data['token'] )\n",
    "\n",
    "result_lr = lr.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['token'] = result_lr\n",
    "for i in range(len(test_data)):\n",
    "    result = {}\n",
    "    result['id'] = str(test_data['id'][i])\n",
    "    result['sample'] = test_data['sample'][i]\n",
    "    result['token'] = test_data['token'][i]\n",
    "    if i < 2399:\n",
    "        with open('w2v_avg_lr.json', 'a') as f:\n",
    "            json.dump(result, f)\n",
    "            f.write('\\n')\n",
    "    else:\n",
    "        with open('w2v_avg_lr.json', 'a') as f:\n",
    "            json.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 1.58 s, total: 1min 7s\n",
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.cluster import KMeans\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip(model.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (labeled_train_data[\"sample\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for sample in clean_train_sample:\n",
    "    train_centroids[counter] = create_bag_of_centroids( sample, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test_data[\"sample\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for sample in clean_test_sample:\n",
    "    test_centroids[counter] = create_bag_of_centroids( sample, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest and extract predictions \n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit(train_centroids, labeled_train_data['token'])\n",
    "result = forest.predict(test_centroids)\n",
    "test_data['token'] = result\n",
    "for i in range(len(test_data)):\n",
    "    result = {}\n",
    "    result['id'] = str(test_data['id'][i])\n",
    "    result['sample'] = test_data['sample'][i]\n",
    "    result['token'] = test_data['token'][i]\n",
    "    if i < 2399:\n",
    "        with open('w2v_centr_rf.json', 'a') as f:\n",
    "            json.dump(result, f)\n",
    "            f.write('\\n')\n",
    "    else:\n",
    "        with open('w2v_centr_rf.json', 'a') as f:\n",
    "            json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.607083333333"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
